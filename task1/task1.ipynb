{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb9fae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "'''\n",
    "train_data = pd.read_csv('train.tsv', sep='\\t')\n",
    "\n",
    "train_phrases = train_data['Phrase']\n",
    "train_sentiments = train_data['Sentiment']\n",
    "\n",
    "dev_data = pd.read_csv('test.tsv', sep='\\t')\n",
    "\n",
    "dev_phrases = train_data['Phrase']\n",
    "dev_sentiments = train_data['Sentiment']\n",
    "\n",
    "'''\n",
    "\n",
    "train_data = pd.read_csv('new_train.tsv', sep='\\t', header=None, names=['Phrase','Sentiment'])\n",
    "\n",
    "train_phrases = train_data['Phrase']\n",
    "train_sentiments = train_data['Sentiment']\n",
    "\n",
    "dev_data = pd.read_csv('new_test.tsv', sep='\\t', header=None, names=['Phrase','Sentiment'])\n",
    "\n",
    "dev_phrases = dev_data['Phrase']\n",
    "dev_sentiments = dev_data['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff108df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(phrase):\n",
    "    phrase = re.sub(r'[^\\w\\s]', '', phrase).lower()\n",
    "    return phrase.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9916f11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef build_bow_vectors(phrases, vocab=None):\\n    # 构建词汇表\\n    if vocab is None:\\n        all_words = set()\\n        for phrase in phrases:\\n            words = tokenize(phrase)\\n            all_words.update(words)\\n        vocab = list(all_words)\\n    \\n    vocab_size = len(vocab)\\n    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\\n    \\n    # 创建向量\\n    vectors = []\\n    for phrase in phrases:\\n        words = tokenize(phrase)\\n        vector = np.zeros(vocab_size)\\n        for word in words:\\n            if word in word_to_idx:\\n                vector[word_to_idx[word]] += 1\\n        vectors.append(vector)\\n    return np.array(vectors), vocab\\n    '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def build_bow_vectors(phrases, vocab=None):\n",
    "    # 构建词汇表\n",
    "    if vocab is None:\n",
    "        all_words = set()\n",
    "        for phrase in phrases:\n",
    "            words = tokenize(phrase)\n",
    "            all_words.update(words)\n",
    "        vocab = list(all_words)\n",
    "    \n",
    "    vocab_size = len(vocab)\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    # 创建向量\n",
    "    vectors = []\n",
    "    for phrase in phrases:\n",
    "        words = tokenize(phrase)\n",
    "        vector = np.zeros(vocab_size)\n",
    "        for word in words:\n",
    "            if word in word_to_idx:\n",
    "                vector[word_to_idx[word]] += 1\n",
    "        vectors.append(vector)\n",
    "    return np.array(vectors), vocab\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f93e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-Words向量化\n",
    "from scipy import sparse\n",
    "\n",
    "# Bag-of-Words向量化（稀疏矩阵）\n",
    "def build_bow_vectors(phrases, vocab=None):\n",
    "    if vocab is None:\n",
    "        all_words = set()\n",
    "        for phrase in phrases:\n",
    "            words = tokenize(phrase)\n",
    "            all_words.update(words)\n",
    "        vocab = list(all_words)\n",
    "    \n",
    "    vocab_size = len(vocab)\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    \n",
    "    for i, phrase in enumerate(phrases):\n",
    "        words = tokenize(phrase)\n",
    "        for word in words:\n",
    "            if word in word_to_idx:\n",
    "                row.append(i)\n",
    "                col.append(word_to_idx[word])\n",
    "                data.append(1)\n",
    "    \n",
    "    vectors = sparse.csr_matrix((data, (row, col)), shape=(len(phrases), vocab_size), dtype=np.float64)\n",
    "    return vectors, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e17e2469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# N-gram向量化（以二元组为例）\\ndef build_ngram_vectors(phrases, vocab=None, n=3):\\n    # 构建N-gram词汇表\\n    if vocab is None:\\n        all_ngrams = set()\\n        for phrase in phrases:\\n            words = tokenize(phrase)\\n            if(len(words)<n):\\n                continue\\n            for i in range(len(words) - n + 1):\\n                ngram = tuple(words[i:i+n])\\n                all_ngrams.add(ngram)\\n        vocab = list(all_ngrams)\\n    \\n    vocab_size = len(vocab)\\n    ngram_to_idx = {ngram: idx for idx, ngram in enumerate(vocab)}\\n    \\n    # 创建向量\\n    vectors = []\\n    for phrase in phrases:\\n        words = tokenize(phrase)\\n        vector = np.zeros(vocab_size)\\n        for i in range(len(words) - n + 1):\\n            ngram = tuple(words[i:i+n])\\n            if ngram in ngram_to_idx:\\n                vector[ngram_to_idx[ngram]] += 1\\n        vectors.append(vector)\\n    return np.array(vectors), vocab\\n    '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# N-gram向量化（以二元组为例）\n",
    "def build_ngram_vectors(phrases, vocab=None, n=3):\n",
    "    # 构建N-gram词汇表\n",
    "    if vocab is None:\n",
    "        all_ngrams = set()\n",
    "        for phrase in phrases:\n",
    "            words = tokenize(phrase)\n",
    "            if(len(words)<n):\n",
    "                continue\n",
    "            for i in range(len(words) - n + 1):\n",
    "                ngram = tuple(words[i:i+n])\n",
    "                all_ngrams.add(ngram)\n",
    "        vocab = list(all_ngrams)\n",
    "    \n",
    "    vocab_size = len(vocab)\n",
    "    ngram_to_idx = {ngram: idx for idx, ngram in enumerate(vocab)}\n",
    "    \n",
    "    # 创建向量\n",
    "    vectors = []\n",
    "    for phrase in phrases:\n",
    "        words = tokenize(phrase)\n",
    "        vector = np.zeros(vocab_size)\n",
    "        for i in range(len(words) - n + 1):\n",
    "            ngram = tuple(words[i:i+n])\n",
    "            if ngram in ngram_to_idx:\n",
    "                vector[ngram_to_idx[ngram]] += 1\n",
    "        vectors.append(vector)\n",
    "    return np.array(vectors), vocab\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "081c9174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram向量化（稀疏矩阵）\n",
    "def build_ngram_vectors(phrases, vocab=None, max_n=2):\n",
    "    if vocab is None:\n",
    "        all_ngrams = set()\n",
    "        for phrase in phrases:\n",
    "            words = tokenize(phrase)\n",
    "            for n in range(1, max_n + 1):\n",
    "                for i in range(len(words) - n + 1):\n",
    "                    ngram = tuple(words[i:i+n])\n",
    "                    all_ngrams.add(ngram)\n",
    "        vocab = list(all_ngrams)\n",
    "    \n",
    "    vocab_size = len(vocab)\n",
    "    ngram_to_idx = {ngram: idx for idx, ngram in enumerate(vocab)}\n",
    "    \n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    \n",
    "    for i, phrase in enumerate(phrases):\n",
    "        words = tokenize(phrase)\n",
    "        for n in range(1, max_n + 1):\n",
    "            for j in range(len(words) - n + 1):\n",
    "                ngram = tuple(words[j:j+n])\n",
    "                if ngram in ngram_to_idx:\n",
    "                    row.append(i)\n",
    "                    col.append(ngram_to_idx[ngram])\n",
    "                    data.append(1)\n",
    "    \n",
    "    vectors = sparse.csr_matrix((data, (row, col)), shape=(len(phrases), vocab_size), dtype=np.float64)\n",
    "    return vectors, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22af9bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax函数\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / exp_z.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "# 逻辑回归训练\n",
    "def train_softmax_regression(X, y, num_classes=5, learning_rate=0.1, epochs=100):\n",
    "    num_samples, num_features = X.shape\n",
    "    np.random.seed(42)\n",
    "    W = np.random.randn(num_features, num_classes) * 0.01\n",
    "    b = np.zeros(num_classes)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # 前向传播\n",
    "        Z = X @ W + b\n",
    "        P = softmax(Z)\n",
    "        \n",
    "        # 计算损失\n",
    "        y_onehot = np.zeros((num_samples, num_classes))\n",
    "        y_onehot[np.arange(num_samples), y] = 1\n",
    "        loss = -np.mean(np.sum(y_onehot * np.log(P + 1e-9), axis=1))\n",
    "        \n",
    "        # 反向传播\n",
    "        dZ = P - y_onehot\n",
    "        dW = X.T @ dZ / num_samples\n",
    "        db = np.sum(dZ, axis=0) / num_samples\n",
    "        \n",
    "        # 更新参数\n",
    "        W -= learning_rate * dW\n",
    "        b -= learning_rate * db\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    return W, b\n",
    "\n",
    "# 预测函数\n",
    "def predict(X, W, b):\n",
    "    Z = X @ W + b\n",
    "    P = softmax(Z)\n",
    "    return np.argmax(P, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07bfadc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.6107\n",
      "Epoch 10, Loss: 1.5329\n",
      "Epoch 20, Loss: 1.5085\n",
      "Epoch 30, Loss: 1.4954\n",
      "Epoch 40, Loss: 1.4860\n",
      "Epoch 50, Loss: 1.4784\n",
      "Epoch 60, Loss: 1.4717\n",
      "Epoch 70, Loss: 1.4657\n",
      "Epoch 80, Loss: 1.4602\n",
      "Epoch 90, Loss: 1.4552\n",
      "Epoch 100, Loss: 1.4504\n",
      "Epoch 110, Loss: 1.4459\n",
      "Epoch 120, Loss: 1.4417\n",
      "Epoch 130, Loss: 1.4377\n",
      "Epoch 140, Loss: 1.4338\n",
      "Epoch 150, Loss: 1.4302\n",
      "Epoch 160, Loss: 1.4266\n",
      "Epoch 170, Loss: 1.4232\n",
      "Epoch 180, Loss: 1.4199\n",
      "Epoch 190, Loss: 1.4168\n",
      "Epoch 200, Loss: 1.4137\n",
      "Epoch 210, Loss: 1.4107\n",
      "Epoch 220, Loss: 1.4078\n",
      "Epoch 230, Loss: 1.4050\n",
      "Epoch 240, Loss: 1.4023\n",
      "Epoch 250, Loss: 1.3996\n",
      "Epoch 260, Loss: 1.3970\n",
      "Epoch 270, Loss: 1.3945\n",
      "Epoch 280, Loss: 1.3920\n",
      "Epoch 290, Loss: 1.3895\n",
      "Epoch 300, Loss: 1.3871\n",
      "Epoch 310, Loss: 1.3848\n",
      "Epoch 320, Loss: 1.3825\n",
      "Epoch 330, Loss: 1.3802\n",
      "Epoch 340, Loss: 1.3780\n",
      "Epoch 350, Loss: 1.3758\n",
      "Epoch 360, Loss: 1.3736\n",
      "Epoch 370, Loss: 1.3715\n",
      "Epoch 380, Loss: 1.3694\n",
      "Epoch 390, Loss: 1.3674\n",
      "Epoch 400, Loss: 1.3653\n",
      "Epoch 410, Loss: 1.3633\n",
      "Epoch 420, Loss: 1.3614\n",
      "Epoch 430, Loss: 1.3594\n",
      "Epoch 440, Loss: 1.3575\n",
      "Epoch 450, Loss: 1.3556\n",
      "Epoch 460, Loss: 1.3537\n",
      "Epoch 470, Loss: 1.3519\n",
      "Epoch 480, Loss: 1.3500\n",
      "Epoch 490, Loss: 1.3482\n",
      "Epoch 500, Loss: 1.3464\n",
      "Epoch 510, Loss: 1.3446\n",
      "Epoch 520, Loss: 1.3429\n",
      "Epoch 530, Loss: 1.3412\n",
      "Epoch 540, Loss: 1.3394\n",
      "Epoch 550, Loss: 1.3377\n",
      "Epoch 560, Loss: 1.3360\n",
      "Epoch 570, Loss: 1.3344\n",
      "Epoch 580, Loss: 1.3327\n",
      "Epoch 590, Loss: 1.3311\n",
      "Epoch 600, Loss: 1.3294\n",
      "Epoch 610, Loss: 1.3278\n",
      "Epoch 620, Loss: 1.3262\n",
      "Epoch 630, Loss: 1.3246\n",
      "Epoch 640, Loss: 1.3231\n",
      "Epoch 650, Loss: 1.3215\n",
      "Epoch 660, Loss: 1.3200\n",
      "Epoch 670, Loss: 1.3184\n",
      "Epoch 680, Loss: 1.3169\n",
      "Epoch 690, Loss: 1.3154\n",
      "Epoch 700, Loss: 1.3139\n",
      "Epoch 710, Loss: 1.3124\n",
      "Epoch 720, Loss: 1.3109\n",
      "Epoch 730, Loss: 1.3094\n",
      "Epoch 740, Loss: 1.3080\n",
      "Epoch 750, Loss: 1.3065\n",
      "Epoch 760, Loss: 1.3051\n",
      "Epoch 770, Loss: 1.3037\n",
      "Epoch 780, Loss: 1.3022\n",
      "Epoch 790, Loss: 1.3008\n",
      "Epoch 800, Loss: 1.2994\n",
      "Epoch 810, Loss: 1.2980\n",
      "Epoch 820, Loss: 1.2967\n",
      "Epoch 830, Loss: 1.2953\n",
      "Epoch 840, Loss: 1.2939\n",
      "Epoch 850, Loss: 1.2926\n",
      "Epoch 860, Loss: 1.2912\n",
      "Epoch 870, Loss: 1.2899\n",
      "Epoch 880, Loss: 1.2885\n",
      "Epoch 890, Loss: 1.2872\n",
      "Epoch 900, Loss: 1.2859\n",
      "Epoch 910, Loss: 1.2846\n",
      "Epoch 920, Loss: 1.2833\n",
      "Epoch 930, Loss: 1.2820\n",
      "Epoch 940, Loss: 1.2807\n",
      "Epoch 950, Loss: 1.2794\n",
      "Epoch 960, Loss: 1.2781\n",
      "Epoch 970, Loss: 1.2769\n",
      "Epoch 980, Loss: 1.2756\n",
      "Epoch 990, Loss: 1.2744\n",
      "Epoch 1000, Loss: 1.2731\n",
      "Epoch 1010, Loss: 1.2719\n",
      "Epoch 1020, Loss: 1.2706\n",
      "Epoch 1030, Loss: 1.2694\n",
      "Epoch 1040, Loss: 1.2682\n",
      "Epoch 1050, Loss: 1.2670\n",
      "Epoch 1060, Loss: 1.2658\n",
      "Epoch 1070, Loss: 1.2646\n",
      "Epoch 1080, Loss: 1.2634\n",
      "Epoch 1090, Loss: 1.2622\n",
      "Epoch 1100, Loss: 1.2610\n",
      "Epoch 1110, Loss: 1.2598\n",
      "Epoch 1120, Loss: 1.2586\n",
      "Epoch 1130, Loss: 1.2574\n",
      "Epoch 1140, Loss: 1.2563\n",
      "Epoch 1150, Loss: 1.2551\n",
      "Epoch 1160, Loss: 1.2540\n",
      "Epoch 1170, Loss: 1.2528\n",
      "Epoch 1180, Loss: 1.2517\n",
      "Epoch 1190, Loss: 1.2505\n",
      "Epoch 1200, Loss: 1.2494\n",
      "Epoch 1210, Loss: 1.2483\n",
      "Epoch 1220, Loss: 1.2472\n",
      "Epoch 1230, Loss: 1.2460\n",
      "Epoch 1240, Loss: 1.2449\n",
      "Epoch 1250, Loss: 1.2438\n",
      "Epoch 1260, Loss: 1.2427\n",
      "Epoch 1270, Loss: 1.2416\n",
      "Epoch 1280, Loss: 1.2405\n",
      "Epoch 1290, Loss: 1.2394\n",
      "Epoch 1300, Loss: 1.2383\n",
      "Epoch 1310, Loss: 1.2373\n",
      "Epoch 1320, Loss: 1.2362\n",
      "Epoch 1330, Loss: 1.2351\n",
      "Epoch 1340, Loss: 1.2340\n",
      "Epoch 1350, Loss: 1.2330\n",
      "Epoch 1360, Loss: 1.2319\n",
      "Epoch 1370, Loss: 1.2309\n",
      "Epoch 1380, Loss: 1.2298\n",
      "Epoch 1390, Loss: 1.2288\n",
      "Epoch 1400, Loss: 1.2277\n",
      "Epoch 1410, Loss: 1.2267\n",
      "Epoch 1420, Loss: 1.2256\n",
      "Epoch 1430, Loss: 1.2246\n",
      "Epoch 1440, Loss: 1.2236\n",
      "Epoch 1450, Loss: 1.2225\n",
      "Epoch 1460, Loss: 1.2215\n",
      "Epoch 1470, Loss: 1.2205\n",
      "Epoch 1480, Loss: 1.2195\n",
      "Epoch 1490, Loss: 1.2185\n",
      "Epoch 1500, Loss: 1.2175\n",
      "Epoch 1510, Loss: 1.2165\n",
      "Epoch 1520, Loss: 1.2155\n",
      "Epoch 1530, Loss: 1.2145\n",
      "Epoch 1540, Loss: 1.2135\n",
      "Epoch 1550, Loss: 1.2125\n",
      "Epoch 1560, Loss: 1.2115\n",
      "Epoch 1570, Loss: 1.2105\n",
      "Epoch 1580, Loss: 1.2096\n",
      "Epoch 1590, Loss: 1.2086\n",
      "Epoch 1600, Loss: 1.2076\n",
      "Epoch 1610, Loss: 1.2067\n",
      "Epoch 1620, Loss: 1.2057\n",
      "Epoch 1630, Loss: 1.2047\n",
      "Epoch 1640, Loss: 1.2038\n",
      "Epoch 1650, Loss: 1.2028\n",
      "Epoch 1660, Loss: 1.2019\n",
      "Epoch 1670, Loss: 1.2009\n",
      "Epoch 1680, Loss: 1.2000\n",
      "Epoch 1690, Loss: 1.1990\n",
      "Epoch 1700, Loss: 1.1981\n",
      "Epoch 1710, Loss: 1.1972\n",
      "Epoch 1720, Loss: 1.1962\n",
      "Epoch 1730, Loss: 1.1953\n",
      "Epoch 1740, Loss: 1.1944\n",
      "Epoch 1750, Loss: 1.1935\n",
      "Epoch 1760, Loss: 1.1925\n",
      "Epoch 1770, Loss: 1.1916\n",
      "Epoch 1780, Loss: 1.1907\n",
      "Epoch 1790, Loss: 1.1898\n",
      "Epoch 1800, Loss: 1.1889\n",
      "Epoch 1810, Loss: 1.1880\n",
      "Epoch 1820, Loss: 1.1871\n",
      "Epoch 1830, Loss: 1.1862\n",
      "Epoch 1840, Loss: 1.1853\n",
      "Epoch 1850, Loss: 1.1844\n",
      "Epoch 1860, Loss: 1.1835\n",
      "Epoch 1870, Loss: 1.1826\n",
      "Epoch 1880, Loss: 1.1817\n",
      "Epoch 1890, Loss: 1.1808\n",
      "Epoch 1900, Loss: 1.1800\n",
      "Epoch 1910, Loss: 1.1791\n",
      "Epoch 1920, Loss: 1.1782\n",
      "Epoch 1930, Loss: 1.1773\n",
      "Epoch 1940, Loss: 1.1765\n",
      "Epoch 1950, Loss: 1.1756\n",
      "Epoch 1960, Loss: 1.1747\n",
      "Epoch 1970, Loss: 1.1739\n",
      "Epoch 1980, Loss: 1.1730\n",
      "Epoch 1990, Loss: 1.1721\n",
      "Epoch 2000, Loss: 1.1713\n",
      "Epoch 2010, Loss: 1.1704\n",
      "Epoch 2020, Loss: 1.1696\n",
      "Epoch 2030, Loss: 1.1687\n",
      "Epoch 2040, Loss: 1.1679\n",
      "Epoch 2050, Loss: 1.1671\n",
      "Epoch 2060, Loss: 1.1662\n",
      "Epoch 2070, Loss: 1.1654\n",
      "Epoch 2080, Loss: 1.1645\n",
      "Epoch 2090, Loss: 1.1637\n",
      "Epoch 2100, Loss: 1.1629\n",
      "Epoch 2110, Loss: 1.1620\n",
      "Epoch 2120, Loss: 1.1612\n",
      "Epoch 2130, Loss: 1.1604\n",
      "Epoch 2140, Loss: 1.1596\n",
      "Epoch 2150, Loss: 1.1588\n",
      "Epoch 2160, Loss: 1.1579\n",
      "Epoch 2170, Loss: 1.1571\n",
      "Epoch 2180, Loss: 1.1563\n",
      "Epoch 2190, Loss: 1.1555\n",
      "Epoch 2200, Loss: 1.1547\n",
      "Epoch 2210, Loss: 1.1539\n",
      "Epoch 2220, Loss: 1.1531\n",
      "Epoch 2230, Loss: 1.1523\n",
      "Epoch 2240, Loss: 1.1515\n",
      "Epoch 2250, Loss: 1.1507\n",
      "Epoch 2260, Loss: 1.1499\n",
      "Epoch 2270, Loss: 1.1491\n",
      "Epoch 2280, Loss: 1.1483\n",
      "Epoch 2290, Loss: 1.1475\n",
      "Epoch 2300, Loss: 1.1467\n",
      "Epoch 2310, Loss: 1.1459\n",
      "Epoch 2320, Loss: 1.1451\n",
      "Epoch 2330, Loss: 1.1444\n",
      "Epoch 2340, Loss: 1.1436\n",
      "Epoch 2350, Loss: 1.1428\n",
      "Epoch 2360, Loss: 1.1420\n",
      "Epoch 2370, Loss: 1.1413\n",
      "Epoch 2380, Loss: 1.1405\n",
      "Epoch 2390, Loss: 1.1397\n",
      "Epoch 2400, Loss: 1.1389\n",
      "Epoch 2410, Loss: 1.1382\n",
      "Epoch 2420, Loss: 1.1374\n",
      "Epoch 2430, Loss: 1.1366\n",
      "Epoch 2440, Loss: 1.1359\n",
      "Epoch 2450, Loss: 1.1351\n",
      "Epoch 2460, Loss: 1.1344\n",
      "Epoch 2470, Loss: 1.1336\n",
      "Epoch 2480, Loss: 1.1329\n",
      "Epoch 2490, Loss: 1.1321\n",
      "Epoch 2500, Loss: 1.1314\n",
      "Epoch 2510, Loss: 1.1306\n",
      "Epoch 2520, Loss: 1.1299\n",
      "Epoch 2530, Loss: 1.1291\n",
      "Epoch 2540, Loss: 1.1284\n",
      "Epoch 2550, Loss: 1.1276\n",
      "Epoch 2560, Loss: 1.1269\n",
      "Epoch 2570, Loss: 1.1262\n",
      "Epoch 2580, Loss: 1.1254\n",
      "Epoch 2590, Loss: 1.1247\n",
      "Epoch 2600, Loss: 1.1240\n",
      "Epoch 2610, Loss: 1.1232\n",
      "Epoch 2620, Loss: 1.1225\n",
      "Epoch 2630, Loss: 1.1218\n",
      "Epoch 2640, Loss: 1.1211\n",
      "Epoch 2650, Loss: 1.1203\n",
      "Epoch 2660, Loss: 1.1196\n",
      "Epoch 2670, Loss: 1.1189\n",
      "Epoch 2680, Loss: 1.1182\n",
      "Epoch 2690, Loss: 1.1175\n",
      "Epoch 2700, Loss: 1.1167\n",
      "Epoch 2710, Loss: 1.1160\n",
      "Epoch 2720, Loss: 1.1153\n",
      "Epoch 2730, Loss: 1.1146\n",
      "Epoch 2740, Loss: 1.1139\n",
      "Epoch 2750, Loss: 1.1132\n",
      "Epoch 2760, Loss: 1.1125\n",
      "Epoch 2770, Loss: 1.1118\n",
      "Epoch 2780, Loss: 1.1111\n",
      "Epoch 2790, Loss: 1.1104\n",
      "Epoch 2800, Loss: 1.1097\n",
      "Epoch 2810, Loss: 1.1090\n",
      "Epoch 2820, Loss: 1.1083\n",
      "Epoch 2830, Loss: 1.1076\n",
      "Epoch 2840, Loss: 1.1069\n",
      "Epoch 2850, Loss: 1.1062\n",
      "Epoch 2860, Loss: 1.1055\n",
      "Epoch 2870, Loss: 1.1048\n",
      "Epoch 2880, Loss: 1.1041\n",
      "Epoch 2890, Loss: 1.1035\n",
      "Epoch 2900, Loss: 1.1028\n",
      "Epoch 2910, Loss: 1.1021\n",
      "Epoch 2920, Loss: 1.1014\n",
      "Epoch 2930, Loss: 1.1007\n",
      "Epoch 2940, Loss: 1.1001\n",
      "Epoch 2950, Loss: 1.0994\n",
      "Epoch 2960, Loss: 1.0987\n",
      "Epoch 2970, Loss: 1.0980\n",
      "Epoch 2980, Loss: 1.0974\n",
      "Epoch 2990, Loss: 1.0967\n",
      "Epoch 3000, Loss: 1.0960\n",
      "Epoch 3010, Loss: 1.0954\n",
      "Epoch 3020, Loss: 1.0947\n",
      "Epoch 3030, Loss: 1.0940\n",
      "Epoch 3040, Loss: 1.0934\n",
      "Epoch 3050, Loss: 1.0927\n",
      "Epoch 3060, Loss: 1.0920\n",
      "Epoch 3070, Loss: 1.0914\n",
      "Epoch 3080, Loss: 1.0907\n",
      "Epoch 3090, Loss: 1.0901\n",
      "Epoch 3100, Loss: 1.0894\n",
      "Epoch 3110, Loss: 1.0887\n",
      "Epoch 3120, Loss: 1.0881\n",
      "Epoch 3130, Loss: 1.0874\n",
      "Epoch 3140, Loss: 1.0868\n",
      "Epoch 3150, Loss: 1.0861\n",
      "Epoch 3160, Loss: 1.0855\n",
      "Epoch 3170, Loss: 1.0849\n",
      "Epoch 3180, Loss: 1.0842\n",
      "Epoch 3190, Loss: 1.0836\n",
      "Epoch 3200, Loss: 1.0829\n",
      "Epoch 3210, Loss: 1.0823\n",
      "Epoch 3220, Loss: 1.0816\n",
      "Epoch 3230, Loss: 1.0810\n",
      "Epoch 3240, Loss: 1.0804\n",
      "Epoch 3250, Loss: 1.0797\n",
      "Epoch 3260, Loss: 1.0791\n",
      "Epoch 3270, Loss: 1.0785\n",
      "Epoch 3280, Loss: 1.0778\n",
      "Epoch 3290, Loss: 1.0772\n",
      "Epoch 3300, Loss: 1.0766\n",
      "Epoch 3310, Loss: 1.0759\n",
      "Epoch 3320, Loss: 1.0753\n",
      "Epoch 3330, Loss: 1.0747\n",
      "Epoch 3340, Loss: 1.0741\n",
      "Epoch 3350, Loss: 1.0734\n",
      "Epoch 3360, Loss: 1.0728\n",
      "Epoch 3370, Loss: 1.0722\n",
      "Epoch 3380, Loss: 1.0716\n",
      "Epoch 3390, Loss: 1.0710\n",
      "Epoch 3400, Loss: 1.0703\n",
      "Epoch 3410, Loss: 1.0697\n",
      "Epoch 3420, Loss: 1.0691\n",
      "Epoch 3430, Loss: 1.0685\n",
      "Epoch 3440, Loss: 1.0679\n",
      "Epoch 3450, Loss: 1.0673\n",
      "Epoch 3460, Loss: 1.0667\n",
      "Epoch 3470, Loss: 1.0661\n",
      "Epoch 3480, Loss: 1.0654\n",
      "Epoch 3490, Loss: 1.0648\n",
      "Epoch 3500, Loss: 1.0642\n",
      "Epoch 3510, Loss: 1.0636\n",
      "Epoch 3520, Loss: 1.0630\n",
      "Epoch 3530, Loss: 1.0624\n",
      "Epoch 3540, Loss: 1.0618\n",
      "Epoch 3550, Loss: 1.0612\n",
      "Epoch 3560, Loss: 1.0606\n",
      "Epoch 3570, Loss: 1.0600\n",
      "Epoch 3580, Loss: 1.0594\n",
      "Epoch 3590, Loss: 1.0588\n",
      "Epoch 3600, Loss: 1.0582\n",
      "Epoch 3610, Loss: 1.0576\n",
      "Epoch 3620, Loss: 1.0570\n",
      "Epoch 3630, Loss: 1.0565\n",
      "Epoch 3640, Loss: 1.0559\n",
      "Epoch 3650, Loss: 1.0553\n",
      "Epoch 3660, Loss: 1.0547\n",
      "Epoch 3670, Loss: 1.0541\n",
      "Epoch 3680, Loss: 1.0535\n",
      "Epoch 3690, Loss: 1.0529\n",
      "Epoch 3700, Loss: 1.0523\n",
      "Epoch 3710, Loss: 1.0518\n",
      "Epoch 3720, Loss: 1.0512\n",
      "Epoch 3730, Loss: 1.0506\n",
      "Epoch 3740, Loss: 1.0500\n",
      "Epoch 3750, Loss: 1.0494\n",
      "Epoch 3760, Loss: 1.0489\n",
      "Epoch 3770, Loss: 1.0483\n",
      "Epoch 3780, Loss: 1.0477\n",
      "Epoch 3790, Loss: 1.0471\n",
      "Epoch 3800, Loss: 1.0466\n",
      "Epoch 3810, Loss: 1.0460\n",
      "Epoch 3820, Loss: 1.0454\n",
      "Epoch 3830, Loss: 1.0449\n",
      "Epoch 3840, Loss: 1.0443\n",
      "Epoch 3850, Loss: 1.0437\n",
      "Epoch 3860, Loss: 1.0431\n",
      "Epoch 3870, Loss: 1.0426\n",
      "Epoch 3880, Loss: 1.0420\n",
      "Epoch 3890, Loss: 1.0415\n",
      "Epoch 3900, Loss: 1.0409\n",
      "Epoch 3910, Loss: 1.0403\n",
      "Epoch 3920, Loss: 1.0398\n",
      "Epoch 3930, Loss: 1.0392\n",
      "Epoch 3940, Loss: 1.0386\n",
      "Epoch 3950, Loss: 1.0381\n",
      "Epoch 3960, Loss: 1.0375\n",
      "Epoch 3970, Loss: 1.0370\n",
      "Epoch 3980, Loss: 1.0364\n",
      "Epoch 3990, Loss: 1.0359\n",
      "[3 1 1 ... 4 1 1]\n",
      "Bag-of-Words Accuracy: 0.4817\n"
     ]
    }
   ],
   "source": [
    "# Bag-of-Words\n",
    "train_bow, bow_vocab = build_bow_vectors(train_phrases)\n",
    "dev_bow, ngram_vocab = build_bow_vectors(dev_phrases, bow_vocab)\n",
    "\n",
    "# 训练和验证BoW模型\n",
    "W_bow, b_bow = train_softmax_regression(train_bow, train_sentiments, epochs=4000)\n",
    "pred_bow = predict(dev_bow, W_bow, b_bow)\n",
    "print(pred_bow)\n",
    "accuracy_bow = np.mean(pred_bow == dev_sentiments)\n",
    "print(f\"Bag-of-Words Accuracy: {accuracy_bow:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e26c7500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.6109\n",
      "Epoch 10, Loss: 1.5297\n",
      "Epoch 20, Loss: 1.5021\n",
      "Epoch 30, Loss: 1.4857\n",
      "Epoch 40, Loss: 1.4730\n",
      "Epoch 50, Loss: 1.4621\n",
      "Epoch 60, Loss: 1.4522\n",
      "Epoch 70, Loss: 1.4431\n",
      "Epoch 80, Loss: 1.4345\n",
      "Epoch 90, Loss: 1.4264\n",
      "Epoch 100, Loss: 1.4187\n",
      "Epoch 110, Loss: 1.4112\n",
      "Epoch 120, Loss: 1.4041\n",
      "Epoch 130, Loss: 1.3972\n",
      "Epoch 140, Loss: 1.3906\n",
      "Epoch 150, Loss: 1.3841\n",
      "Epoch 160, Loss: 1.3779\n",
      "Epoch 170, Loss: 1.3718\n",
      "Epoch 180, Loss: 1.3658\n",
      "Epoch 190, Loss: 1.3600\n",
      "Epoch 200, Loss: 1.3543\n",
      "Epoch 210, Loss: 1.3488\n",
      "Epoch 220, Loss: 1.3433\n",
      "Epoch 230, Loss: 1.3379\n",
      "Epoch 240, Loss: 1.3327\n",
      "Epoch 250, Loss: 1.3275\n",
      "Epoch 260, Loss: 1.3224\n",
      "Epoch 270, Loss: 1.3174\n",
      "Epoch 280, Loss: 1.3125\n",
      "Epoch 290, Loss: 1.3077\n",
      "Epoch 300, Loss: 1.3029\n",
      "Epoch 310, Loss: 1.2982\n",
      "Epoch 320, Loss: 1.2935\n",
      "Epoch 330, Loss: 1.2889\n",
      "Epoch 340, Loss: 1.2844\n",
      "Epoch 350, Loss: 1.2799\n",
      "Epoch 360, Loss: 1.2755\n",
      "Epoch 370, Loss: 1.2711\n",
      "Epoch 380, Loss: 1.2668\n",
      "Epoch 390, Loss: 1.2625\n",
      "Epoch 400, Loss: 1.2583\n",
      "Epoch 410, Loss: 1.2541\n",
      "Epoch 420, Loss: 1.2499\n",
      "Epoch 430, Loss: 1.2458\n",
      "Epoch 440, Loss: 1.2417\n",
      "Epoch 450, Loss: 1.2377\n",
      "Epoch 460, Loss: 1.2337\n",
      "Epoch 470, Loss: 1.2298\n",
      "Epoch 480, Loss: 1.2258\n",
      "Epoch 490, Loss: 1.2220\n",
      "Epoch 500, Loss: 1.2181\n",
      "Epoch 510, Loss: 1.2143\n",
      "Epoch 520, Loss: 1.2105\n",
      "Epoch 530, Loss: 1.2067\n",
      "Epoch 540, Loss: 1.2030\n",
      "Epoch 550, Loss: 1.1993\n",
      "Epoch 560, Loss: 1.1956\n",
      "Epoch 570, Loss: 1.1920\n",
      "Epoch 580, Loss: 1.1884\n",
      "Epoch 590, Loss: 1.1848\n",
      "Epoch 600, Loss: 1.1812\n",
      "Epoch 610, Loss: 1.1777\n",
      "Epoch 620, Loss: 1.1742\n",
      "Epoch 630, Loss: 1.1707\n",
      "Epoch 640, Loss: 1.1673\n",
      "Epoch 650, Loss: 1.1638\n",
      "Epoch 660, Loss: 1.1604\n",
      "Epoch 670, Loss: 1.1570\n",
      "Epoch 680, Loss: 1.1537\n",
      "Epoch 690, Loss: 1.1503\n",
      "[1 1 1 ... 4 1 1]\n",
      "N-gram Accuracy: 0.4518\n"
     ]
    }
   ],
   "source": [
    "# N-gram（二元组）\n",
    "train_ngram, ngram_vocab = build_ngram_vectors(train_phrases, max_n=3)\n",
    "dev_ngram, ngram_vocab = build_ngram_vectors(dev_phrases, vocab=ngram_vocab, max_n=3)\n",
    "\n",
    "# 训练和验证N-gram模型\n",
    "W_ngram, b_ngram = train_softmax_regression(train_ngram, train_sentiments, epochs=700)\n",
    "pred_ngram = predict(dev_ngram, W_ngram, b_ngram)\n",
    "print(pred_ngram)\n",
    "accuracy_ngram = np.mean(pred_ngram == dev_sentiments)\n",
    "print(f\"N-gram Accuracy: {accuracy_ngram:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
